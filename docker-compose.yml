services:
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      cache_from:
        - node:20-alpine
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL}
      - WATCHPACK_POLLING=true
      - CHOKIDAR_USEPOLLING=true
      - TURBOPACK=0
    working_dir: /app
    env_file:
      - .env
    volumes:
      # Mount source code for hot reloading
      - ./frontend:/app
      - /app/node_modules
      - /app/.next
    depends_on:
      - api-gateway
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

  api-gateway:
    build:
      context: ./api-gateway
      dockerfile: Dockerfile
    ports:
      - "7070:7070"
    environment:
      - PUBLIC_WEB_ORIGIN=${PUBLIC_WEB_ORIGIN}
      - ORCHESTRATOR_URL=http://orchestrator:8080
      - CVE_INGESTOR_URL=http://cve-ingestor:9095
      - AI_RAG_URL=http://ai-rag:9090
    env_file:
      - .env
    depends_on:
      - orchestrator
      - cve-ingestor
      - ai-rag
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  orchestrator:
    build:
      context: ./orchestrator/Orchestrator
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - JAWSDB_URL=${JAWSDB_URL}
      - PUBLIC_WEB_ORIGIN=${PUBLIC_WEB_ORIGIN}
      - AI_RAG_URL=http://ai-rag:9090
      - ETL_NV_URL=http://etl-nv:9101
      - ETL_V_URL=http://etl-v:9102
    env_file:
      - .env
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

  ai-rag:
    build:
      context: ./ai-rag
      dockerfile: Dockerfile
    ports:
      - "9090:9090"
    environment:
      - OLLAMA_URL=${OLLAMA_URL}
      - GEN_MODEL=${GEN_MODEL}
      - EMBED_MODEL=${EMBED_MODEL}
      - PINECONE_API_KEY=${PINECONE_API_KEY}
      - PINECONE_ENVIRONMENT=${PINECONE_ENVIRONMENT}
      - PINECONE_INDEX=${PINECONE_INDEX}
      - RAG_SERVICE_KEY=${RAG_SERVICE_KEY}
      - API_SIGNING_SECRET=${API_SIGNING_SECRET}
    env_file:
      - .env
    depends_on:
      - ollama
    deploy:
      resources:
        limits:
          cpus: '1.0'  # Reduced - AI-RAG is just a proxy, doesn't need much
          memory: 1G  # Reduced - AI-RAG is lightweight
        reservations:
          cpus: '0.25'  # Minimal reservation
          memory: 256M  # Minimal reservation

  etl-nv:
    build:
      context: ./etl-nv
      dockerfile: Dockerfile
    ports:
      - "9101:9101"
    environment:
      - JAWSDB_NV_URL=${JAWSDB_NV_URL}
    env_file:
      - .env
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  etl-v:
    build:
      context: ./etl-v
      dockerfile: Dockerfile
    ports:
      - "9102:9102"
    environment:
      - JAWSDB_V_URL=${JAWSDB_V_URL}
    env_file:
      - .env
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_cache:/root/.ollama
    # Docker performance optimizations
    shm_size: '2gb'  # Increase shared memory for better performance
    network_mode: bridge  # Use bridge network for better performance
    ulimits:
      memlock: -1  # Unlimited memory locking (helps with model loading)
      nofile:
        soft: 65536  # Increase file descriptor limit
        hard: 65536
    environment:
      # Performance optimizations - MAXIMUM resources for speed (Ollama is critical)
      - OLLAMA_NUM_PARALLEL=1  # Process one request at a time (prevents overload)
      - OLLAMA_MAX_LOADED=3  # Keep 3 models loaded (generation + embedding + backup) - matches docs format
      - OLLAMA_KEEP_ALIVE=-1  # Keep models loaded in memory indefinitely (prevents reload delays)
      - OLLAMA_NUM_THREADS=8  # Use 8 threads to match 8 CPU limit (maximum parallelization) - matches docs format
      - OLLAMA_CONTEXT_SIZE=2048  # Smaller context window = faster generation (default is 4096)
      - OLLAMA_HOST=0.0.0.0  # Listen on all interfaces
      - OLLAMA_CUDA=1  # Enable GPU acceleration if available (will fallback to CPU if no GPU)
    deploy:
      resources:
        limits:
          cpus: '8.0'  # MAXIMUM CPU - Ollama is the most critical service, needs all available CPU
          memory: 12G  # MAXIMUM MEMORY - Keep models loaded and allow fast inference without swapping
        reservations:
          cpus: '2.0'  # High CPU reservation to ensure Ollama always has resources
          memory: 4G  # High memory reservation to keep models loaded
    # Optional: comment out if using local Ollama desktop app

  cve-ingestor:
    build:
      context: ./cve-ingestor
      dockerfile: Dockerfile
    ports:
      - "9095:9095"
    environment:
      - OLLAMA_URL=${OLLAMA_URL}
      - EMBED_MODEL=${EMBED_MODEL}
      - PINECONE_API_KEY=${PINECONE_API_KEY}
      - PINECONE_ENVIRONMENT=${PINECONE_ENVIRONMENT}
      - PINECONE_INDEX=${PINECONE_INDEX}
    env_file:
      - .env
    depends_on:
      - ai-rag
      - ollama
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

volumes:
  ollama_cache:

