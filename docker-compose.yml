services:
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      cache_from:
        - node:20-alpine
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL}
      - WATCHPACK_POLLING=true
      - CHOKIDAR_USEPOLLING=true
      - TURBOPACK=0
    working_dir: /app
    env_file:
      - .env
    volumes:
      # Mount source code for hot reloading
      - ./frontend:/app
      - /app/node_modules
      - /app/.next
    depends_on:
      - api-gateway
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

  api-gateway:
    build:
      context: ./api-gateway
      dockerfile: Dockerfile
    ports:
      - "7070:7070"
    environment:
      - PUBLIC_WEB_ORIGIN=${PUBLIC_WEB_ORIGIN}
      - ORCHESTRATOR_URL=http://orchestrator:8080
      - CVE_INGESTOR_URL=http://cve-ingestor:9095
      - AI_RAG_URL=http://ai-rag:9090
    env_file:
      - .env
    depends_on:
      - orchestrator
      - cve-ingestor
      - ai-rag
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  orchestrator:
    build:
      context: ./orchestrator/Orchestrator
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - JAWSDB_URL=${JAWSDB_URL}
      - PUBLIC_WEB_ORIGIN=${PUBLIC_WEB_ORIGIN}
      - AI_RAG_URL=http://ai-rag:9090
      - ETL_NV_URL=http://etl-nv:9101
      - ETL_V_URL=http://etl-v:9102
    env_file:
      - .env
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

  ai-rag:
    build:
      context: ./ai-rag
      dockerfile: Dockerfile
    ports:
      - "9090:9090"
    environment:
      - OLLAMA_URL=${OLLAMA_URL}
      - GEN_MODEL=${GEN_MODEL}
      - EMBED_MODEL=${EMBED_MODEL}
      - PINECONE_API_KEY=${PINECONE_API_KEY}
      - PINECONE_ENVIRONMENT=${PINECONE_ENVIRONMENT}
      - PINECONE_INDEX=${PINECONE_INDEX}
      - RAG_SERVICE_KEY=${RAG_SERVICE_KEY}
      - API_SIGNING_SECRET=${API_SIGNING_SECRET}
    env_file:
      - .env
    depends_on:
      - ollama
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

  etl-nv:
    build:
      context: ./etl-nv
      dockerfile: Dockerfile
    ports:
      - "9101:9101"
    environment:
      - JAWSDB_NV_URL=${JAWSDB_NV_URL}
    env_file:
      - .env
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  etl-v:
    build:
      context: ./etl-v
      dockerfile: Dockerfile
    ports:
      - "9102:9102"
    environment:
      - JAWSDB_V_URL=${JAWSDB_V_URL}
    env_file:
      - .env
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_cache:/root/.ollama
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '1.0'
          memory: 2G
    # Optional: comment out if using local Ollama desktop app

  cve-ingestor:
    build:
      context: ./cve-ingestor
      dockerfile: Dockerfile
    ports:
      - "9095:9095"
    environment:
      - OLLAMA_URL=${OLLAMA_URL}
      - EMBED_MODEL=${EMBED_MODEL}
      - PINECONE_API_KEY=${PINECONE_API_KEY}
      - PINECONE_ENVIRONMENT=${PINECONE_ENVIRONMENT}
      - PINECONE_INDEX=${PINECONE_INDEX}
    env_file:
      - .env
    depends_on:
      - ai-rag
      - ollama
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

volumes:
  ollama_cache:

